
    
    
    [{"authors":null,"categories":null,"content":"Ahmed is a PhD student at the UKRI CDT in SLT in the Department of Computer Science at the University of Sheffield. He is advised by Prof. Nikolaos Aletras. His research focuses on the interpretability of neural Natural Language Processing (NLP) models and language modelling.\nPrior to starting his PhD, Ahmed worked for a number of years as a ML/Software Engineer at several companies and institutions.\nAhmed holds an MSc in Computer Science from the University of Manchester and a bachelorâ€™s degree in Computer Engineering from the Islamic University of Gaza.\n","date":1701388800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1701388800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Ahmed is a PhD student at the UKRI CDT in SLT in the Department of Computer Science at the University of Sheffield. He is advised by Prof. Nikolaos Aletras. His research focuses on the interpretability of neural Natural Language Processing (NLP) models and language modelling.","tags":null,"title":"Ahmed Alajrami","type":"authors"},{"authors":["Ahmed Alajrami","Katerina Margatina","Nikolaos Aletras"],"categories":null,"content":" ","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701388800,"objectID":"c8f94d4f3077ff6a75fdd596b8dde35b","permalink":"https://aajrami.github.io/publication/alajrami-emnlp-2023/","publishdate":"2023-12-01T00:00:00Z","relpermalink":"/publication/alajrami-emnlp-2023/","section":"publication","summary":"Understanding how and what pre-trained language models (PLMs) learn about language is an open challenge in natural language processing. Previous work has focused on identifying whether they capture semantic and syntactic information, and how the data or the pre-training objective affects their performance. However, to the best of our knowledge, no previous work has specifically examined how information loss in input token characters affects the performance of PLMs. In this study, we address this gap by pre-training language models using small subsets of characters from individual tokens. Surprisingly, we find that pre-training even under extreme settings, i.e. using only one character of each token, the performance retention in standard NLU benchmarks and probing tasks compared to full-token models is high. For instance, a model pre-trained only on single first characters from tokens achieves performance retention of approximately 90% and 77% of the full-token model in SuperGLUE and GLUE tasks, respectively.","tags":[],"title":"Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?","type":"publication"},{"authors":["Ahmed Alajrami","Nikolaos Aletras"],"categories":null,"content":" ","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"3350892e68054ee3f4286a7f973fac7d","permalink":"https://aajrami.github.io/publication/alajrami-acl-2022/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/alajrami-acl-2022/","section":"publication","summary":"Several pre-training objectives, such as masked language modeling (MLM), have been proposed to pre-train language models (e.g. BERT) with the aim of learning better language representations. However, to the best of our knowledge, no previous work so far has investigated how different pre-training objectives affect what BERT learns about linguistics properties. We hypothesize that linguistically motivated objectives such as MLM should help BERT to acquire better linguistic knowledge compared to other non-linguistically motivated objectives that are not intuitive or hard for humans to guess the association between the input and the label to be predicted. To this end, we pre-train BERT with two linguistically motivated objectives and three non-linguistically motivated ones. We then probe for linguistic characteristics encoded in the representation of the resulting models. We find strong evidence that there are only small differences in probing performance between the representations learned by the two different types of objectives. These surprising results question the dominant narrative of linguistically informed pre-training.","tags":[],"title":"How does the pre-training objective affect what large language models learn about linguistic properties?","type":"publication"}]